<!-- sections/deploy.html -->
<div>
  <h2 style="color:#7dd3fc;margin-top:0;">LLM Deployment</h2>
  <p style="color:#9aa6bd;line-height:1.6;font-size:15px;">
    Deployment focuses on serving the model reliably and efficiently. This includes containerization, autoscaling, inference runtimes, and monitoring.
  </p>

  <h3 style="color:#e6eef6;margin-top:14px;">Deployment notes</h3>
  <ul style="color:#9aa6bd;line-height:1.6">
    <li>Consider optimized runtimes: TGI, vLLM, Triton</li>
    <li>Use quantization to save memory and accelerate inference</li>
    <li>Autoscaling and cost controls for production workloads</li>
  </ul>

  <p style="margin-top:12px;color:#9aa6bd">Edit <code>sections/deploy.html</code> to include examples, docker-compose, or cloud snippets.</p>
</div>
